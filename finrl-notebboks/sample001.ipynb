{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1. Install Packages",
   "id": "e39d37fba4e88909"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:20.274088Z",
     "start_time": "2024-09-20T08:41:20.270177Z"
    }
   },
   "source": [
    "## install required packages\n",
    "#!pip install swig\n",
    "#!pip install wrds\n",
    "#!pip install pyportfolioopt\n",
    "## install finrl library\n",
    "## !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:20.366487Z",
     "start_time": "2024-09-20T08:41:20.352224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gymnasium\n",
    "import stable_baselines3\n",
    "import stockstats\n",
    "import alpaca_trade_api\n",
    "import exchange_calendars\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "\n",
    "import itertools"
   ],
   "id": "b804878aaf03e80f",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2. Fetch data",
   "id": "32f190e6c0c8ddb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**OHLCV**: Data downloaded are in the form of OHLCV, corresponding to **open, high, low, close, volume,** respectively. OHLCV is important because they contain most of numerical information of a stock in time series. From OHLCV, traders can get further judgement and prediction like the momentum, people's interest, market trends, etc.",
   "id": "5f5fc228c6a049a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:20.386119Z",
     "start_time": "2024-09-20T08:41:20.382129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRAIN_START_DATE = '2021-01-01'\n",
    "TRAIN_END_DATE = '2023-07-01'\n",
    "TRADE_START_DATE = '2023-07-01'\n",
    "TRADE_END_DATE = '2024-09-15'\n",
    "\n",
    "config_tickers.DOW_30_TICKER\n"
   ],
   "id": "c9fd0b105467bd3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AXP',\n",
       " 'AMGN',\n",
       " 'AAPL',\n",
       " 'BA',\n",
       " 'CAT',\n",
       " 'CSCO',\n",
       " 'CVX',\n",
       " 'GS',\n",
       " 'HD',\n",
       " 'HON',\n",
       " 'IBM',\n",
       " 'INTC',\n",
       " 'JNJ',\n",
       " 'KO',\n",
       " 'JPM',\n",
       " 'MCD',\n",
       " 'MMM',\n",
       " 'MRK',\n",
       " 'MSFT',\n",
       " 'NKE',\n",
       " 'PG',\n",
       " 'TRV',\n",
       " 'UNH',\n",
       " 'CRM',\n",
       " 'VZ',\n",
       " 'V',\n",
       " 'WBA',\n",
       " 'WMT',\n",
       " 'DIS',\n",
       " 'DOW']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:20.909764Z",
     "start_time": "2024-09-20T08:41:20.386119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#aapl_df_yf = yf.download(tickers = \"aapl\", start=TRAIN_START_DATE, end=TRADE_END_DATE)\n",
    "#aapl_df_yf.head()\n",
    "\n",
    "df_raw = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TRADE_END_DATE,\n",
    "                     ticker_list = config_tickers.DOW_30_TICKER).fetch_data()\n"
   ],
   "id": "e9f44017d58b2064",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (27900, 8)\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:20.916176Z",
     "start_time": "2024-09-20T08:41:20.909764Z"
    }
   },
   "cell_type": "code",
   "source": "df_raw.head()",
   "id": "b5d51305df0e6807",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         date        open        high         low       close     volume  \\\n",
       "0  2021-01-04  133.520004  133.610001  126.760002  126.683434  143301900   \n",
       "1  2021-01-04  231.250000  231.250000  223.669998  201.544556    3088200   \n",
       "2  2021-01-04  121.300003  121.800003  116.849998  112.457108    3472100   \n",
       "3  2021-01-04  210.000000  210.199997  202.490005  202.720001   21225600   \n",
       "4  2021-01-04  183.000000  185.979996  180.250000  168.735245    4078300   \n",
       "\n",
       "    tic  day  \n",
       "0  AAPL    0  \n",
       "1  AMGN    0  \n",
       "2   AXP    0  \n",
       "3    BA    0  \n",
       "4   CAT    0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>133.610001</td>\n",
       "      <td>126.760002</td>\n",
       "      <td>126.683434</td>\n",
       "      <td>143301900</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>231.250000</td>\n",
       "      <td>231.250000</td>\n",
       "      <td>223.669998</td>\n",
       "      <td>201.544556</td>\n",
       "      <td>3088200</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>121.300003</td>\n",
       "      <td>121.800003</td>\n",
       "      <td>116.849998</td>\n",
       "      <td>112.457108</td>\n",
       "      <td>3472100</td>\n",
       "      <td>AXP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.199997</td>\n",
       "      <td>202.490005</td>\n",
       "      <td>202.720001</td>\n",
       "      <td>21225600</td>\n",
       "      <td>BA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>185.979996</td>\n",
       "      <td>180.250000</td>\n",
       "      <td>168.735245</td>\n",
       "      <td>4078300</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 3: Preprocess Data\n",
    "We need to check for missing data and do feature engineering to convert the data point into a state.\n",
    "* **Adding technical indicators**. In practical trading, various information needs to be taken into account, such as historical prices, current holding shares, technical indicators, etc. Here, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* **Adding turbulence index**. Risk-aversion reflects whether an investor prefers to protect the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the turbulence index that measures extreme fluctuation of asset price."
   ],
   "id": "a3f31ecad56f5827"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:20.925977Z",
     "start_time": "2024-09-20T08:41:20.916706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_vix=True,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n"
   ],
   "id": "4f28f7734f0cadb2",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:22.670318Z",
     "start_time": "2024-09-20T08:41:20.925977Z"
    }
   },
   "cell_type": "code",
   "source": "processed = fe.preprocess_data(df_raw)",
   "id": "d163754f2e56fff8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Shape of DataFrame:  (929, 8)\n",
      "Successfully added vix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:22.703486Z",
     "start_time": "2024-09-20T08:41:22.670318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)\n",
    "processed_full.head()\n"
   ],
   "id": "6079e118a2abda43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         date   tic        open        high         low       close  \\\n",
       "0  2021-01-04  AAPL  133.520004  133.610001  126.760002  126.683434   \n",
       "1  2021-01-04  AMGN  231.250000  231.250000  223.669998  201.544556   \n",
       "2  2021-01-04   AXP  121.300003  121.800003  116.849998  112.457108   \n",
       "3  2021-01-04    BA  210.000000  210.199997  202.490005  202.720001   \n",
       "4  2021-01-04   CAT  183.000000  185.979996  180.250000  168.735245   \n",
       "\n",
       "        volume  day  macd    boll_ub     boll_lb  rsi_30     cci_30  dx_30  \\\n",
       "0  143301900.0  0.0   0.0  129.68168  125.251494   100.0  66.666667  100.0   \n",
       "1    3088200.0  0.0   0.0  129.68168  125.251494   100.0  66.666667  100.0   \n",
       "2    3472100.0  0.0   0.0  129.68168  125.251494   100.0  66.666667  100.0   \n",
       "3   21225600.0  0.0   0.0  129.68168  125.251494   100.0  66.666667  100.0   \n",
       "4    4078300.0  0.0   0.0  129.68168  125.251494   100.0  66.666667  100.0   \n",
       "\n",
       "   close_30_sma  close_60_sma        vix  turbulence  \n",
       "0    126.683434    126.683434  26.969999         0.0  \n",
       "1    201.544556    201.544556  26.969999         0.0  \n",
       "2    112.457108    112.457108  26.969999         0.0  \n",
       "3    202.720001    202.720001  26.969999         0.0  \n",
       "4    168.735245    168.735245  26.969999         0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>133.610001</td>\n",
       "      <td>126.760002</td>\n",
       "      <td>126.683434</td>\n",
       "      <td>143301900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.68168</td>\n",
       "      <td>125.251494</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>126.683434</td>\n",
       "      <td>126.683434</td>\n",
       "      <td>26.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>231.250000</td>\n",
       "      <td>231.250000</td>\n",
       "      <td>223.669998</td>\n",
       "      <td>201.544556</td>\n",
       "      <td>3088200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.68168</td>\n",
       "      <td>125.251494</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>201.544556</td>\n",
       "      <td>201.544556</td>\n",
       "      <td>26.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>AXP</td>\n",
       "      <td>121.300003</td>\n",
       "      <td>121.800003</td>\n",
       "      <td>116.849998</td>\n",
       "      <td>112.457108</td>\n",
       "      <td>3472100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.68168</td>\n",
       "      <td>125.251494</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>112.457108</td>\n",
       "      <td>112.457108</td>\n",
       "      <td>26.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>BA</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>210.199997</td>\n",
       "      <td>202.490005</td>\n",
       "      <td>202.720001</td>\n",
       "      <td>21225600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.68168</td>\n",
       "      <td>125.251494</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>202.720001</td>\n",
       "      <td>202.720001</td>\n",
       "      <td>26.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>CAT</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>185.979996</td>\n",
       "      <td>180.250000</td>\n",
       "      <td>168.735245</td>\n",
       "      <td>4078300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.68168</td>\n",
       "      <td>125.251494</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>168.735245</td>\n",
       "      <td>168.735245</td>\n",
       "      <td>26.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:22.706364Z",
     "start_time": "2024-09-20T08:41:22.703486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Part 4: Save the Data\n",
    "\n",
    "### Split the data for training and trading"
   ],
   "id": "33ce3ff7bd30709f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:22.718146Z",
     "start_time": "2024-09-20T08:41:22.706364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))"
   ],
   "id": "cd88ee02f58200f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18810\n",
      "9060\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save data to csv file\n",
   "id": "17fab7eed0b4ac58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:22.966933Z",
     "start_time": "2024-09-20T08:41:22.718146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train.to_csv('train_data.csv')\n",
    "trade.to_csv('trade_data.csv')"
   ],
   "id": "58f7b10f0b3e6396",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 4. Build A Market Environment in OpenAI Gym-style\n",
    "\n",
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal.\n",
    "\n",
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ],
   "id": "9db647f43a161c65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ],
   "id": "cd5f5822cdefa9e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:23.003461Z",
     "start_time": "2024-09-20T08:41:22.966933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n"
   ],
   "id": "46eb5c8f05b07a5a",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Construct the environment\n",
    "\n",
    "Calculate and specify the parameters we need for constructing the environment."
   ],
   "id": "e122451c74619faf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:23.009782Z",
     "start_time": "2024-09-20T08:41:23.003461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ],
   "id": "e5d880435ea8e15e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 30, State Space: 301\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment for training",
   "id": "7763822477627b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:23.013884Z",
     "start_time": "2024-09-20T08:41:23.009782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ],
   "id": "e803a9f8f3f7e30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 5. Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ],
   "id": "8a8d8dc5d4570369"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:23.017223Z",
     "start_time": "2024-09-20T08:41:23.013884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ],
   "id": "8d9be481bdcbcd4d",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)",
   "id": "5430756f0600fbad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 1: A2C",
   "id": "96752c153066b198"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:41:23.027273Z",
     "start_time": "2024-09-20T08:41:23.017223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)\n"
   ],
   "id": "b990681e6f5fa4fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:42:56.039971Z",
     "start_time": "2024-09-20T08:41:23.027273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ],
   "id": "f6c26e0815712ee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 491        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -62.4      |\n",
      "|    reward             | -1.1268827 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 4.17       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 517        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.057      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 15.3       |\n",
      "|    reward             | -0.8347393 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.77       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -66.7    |\n",
      "|    reward             | 1.170702 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 4.18     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 544       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.17      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -54.2     |\n",
      "|    reward             | 1.5458887 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.99      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 549        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 75.4       |\n",
      "|    reward             | -0.6514817 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 543        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -63.5      |\n",
      "|    reward             | -2.6963735 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.94       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 545       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -25.8     |\n",
      "|    reward             | 0.9609954 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.728     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 546        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0.0259     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 90.5       |\n",
      "|    reward             | -0.3083457 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.69       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 550         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 4.41        |\n",
      "|    reward             | -0.46138483 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.197       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -46.1      |\n",
      "|    reward             | 0.62882996 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.35       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 555         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -17         |\n",
      "|    reward             | -0.82879573 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.982       |\n",
      "---------------------------------------\n",
      "day: 626, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1107946.62\n",
      "total_reward: 107946.62\n",
      "total_cost: 16456.77\n",
      "total_trades: 11167\n",
      "Sharpe: 0.332\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 1.67e-06   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 76.6       |\n",
      "|    reward             | -0.9949303 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.32       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -40.9      |\n",
      "|    reward             | -1.2556779 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.2        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 559          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 12           |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.9        |\n",
      "|    explained_variance | -0.176       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | 36.7         |\n",
      "|    reward             | -0.061273888 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 1.09         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 560       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0939    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -10.4     |\n",
      "|    reward             | 1.7358767 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.793     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 561        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 43.7       |\n",
      "|    reward             | 0.25073043 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.21       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 561      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0.00036  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -24.7    |\n",
      "|    reward             | 2.107904 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.852    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43         |\n",
      "|    explained_variance | -0.00412    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -113        |\n",
      "|    reward             | -0.80812204 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 7.59        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.0388    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -48.2      |\n",
      "|    reward             | 0.51996046 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.42       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.0207     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 21.6       |\n",
      "|    reward             | -1.2467328 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.898      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.245     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -130      |\n",
      "|    reward             | 3.0859988 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0.00863    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 76.1       |\n",
      "|    reward             | -1.1156316 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | -0.0684    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | 2.76       |\n",
      "|    reward             | 0.38735008 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.0984     |\n",
      "--------------------------------------\n",
      "day: 626, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1006529.64\n",
      "total_reward: 6529.64\n",
      "total_cost: 18379.87\n",
      "total_trades: 10155\n",
      "Sharpe: 0.095\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 54.6      |\n",
      "|    reward             | 0.3513764 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.91      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 566         |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.1       |\n",
      "|    explained_variance | 0.248       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | -28.6       |\n",
      "|    reward             | -0.16136189 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.1         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 566         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.2       |\n",
      "|    explained_variance | 0.0234      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | 208         |\n",
      "|    reward             | -0.55207264 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 24.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 567       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | 0.136     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -36.9     |\n",
      "|    reward             | -2.706221 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 568        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 104        |\n",
      "|    reward             | -0.6795387 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.96       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 568        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | 2.03e-06   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 73.4       |\n",
      "|    reward             | -0.2722651 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.9        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 568        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 19.2       |\n",
      "|    reward             | 0.66007054 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.276      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 568         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.4       |\n",
      "|    explained_variance | 0.0544      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | -35.7       |\n",
      "|    reward             | -0.39382654 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.47        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 569      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -82.8    |\n",
      "|    reward             | 3.152646 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 3.51     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 32        |\n",
      "|    reward             | 1.2248363 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.961     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -0.446    |\n",
      "|    reward             | 0.8586636 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.051     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 571         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.5       |\n",
      "|    explained_variance | -0.0161     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 59.8        |\n",
      "|    reward             | -0.63373774 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.33        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -79.2      |\n",
      "|    reward             | -1.5102199 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.33       |\n",
      "--------------------------------------\n",
      "day: 626, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 824240.13\n",
      "total_reward: -175759.87\n",
      "total_cost: 6501.17\n",
      "total_trades: 9757\n",
      "Sharpe: -0.337\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 572         |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 32          |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 42.9        |\n",
      "|    reward             | -0.40894198 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.31        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 571      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.6    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -18      |\n",
      "|    reward             | 1.077687 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 1.49     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 567         |\n",
      "|    iterations         | 3900        |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 19500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3899        |\n",
      "|    policy_loss        | 46          |\n",
      "|    reward             | -0.19449615 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 1.95        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 567         |\n",
      "|    iterations         | 4000        |\n",
      "|    time_elapsed       | 35          |\n",
      "|    total_timesteps    | 20000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.6       |\n",
      "|    explained_variance | -0.0381     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3999        |\n",
      "|    policy_loss        | -33.2       |\n",
      "|    reward             | 0.029295191 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 1.51        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -28.1      |\n",
      "|    reward             | -0.6184505 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.871      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 90.7       |\n",
      "|    reward             | 0.99633056 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 8.79       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 562      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.8    |\n",
      "|    explained_variance | 0.0431   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -1.52    |\n",
      "|    reward             | 2.011988 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.197    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 561        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.8      |\n",
      "|    explained_variance | 0.00287    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 82.3       |\n",
      "|    reward             | 0.30251566 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.32       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 561       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 2.49      |\n",
      "|    reward             | 1.7845063 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 3.48      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 560       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 73.2      |\n",
      "|    reward             | 0.3658714 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 3.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 560       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -94.3     |\n",
      "|    reward             | 1.9327792 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 6.18      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 559       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | 8.11      |\n",
      "|    reward             | 0.8629539 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.0938    |\n",
      "-------------------------------------\n",
      "day: 626, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 918365.00\n",
      "total_reward: -81635.00\n",
      "total_cost: 7666.27\n",
      "total_trades: 9299\n",
      "Sharpe: -0.152\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 559       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 19.2      |\n",
      "|    reward             | 1.0174992 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.895     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 559         |\n",
      "|    iterations         | 5000        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 25000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.9       |\n",
      "|    explained_variance | -2.38e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4999        |\n",
      "|    policy_loss        | -30.4       |\n",
      "|    reward             | -0.34085727 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 1.55        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 558       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | -90       |\n",
      "|    reward             | -1.333842 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 4.79      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44        |\n",
      "|    explained_variance | -0.00144   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | -102       |\n",
      "|    reward             | 0.29778245 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 7.56       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 21.1       |\n",
      "|    reward             | 0.17342371 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.956      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 555         |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 48          |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.1       |\n",
      "|    explained_variance | -0.205      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -40.7       |\n",
      "|    reward             | -0.40544966 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.997       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -73.7      |\n",
      "|    reward             | -1.1561282 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 2.75       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.1      |\n",
      "|    explained_variance | 0.0592     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | 123        |\n",
      "|    reward             | -0.8936832 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 8.86      |\n",
      "|    reward             | -0.736883 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.76      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 554         |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 52          |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | 18          |\n",
      "|    reward             | -0.24284442 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.236       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 553         |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.3       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 58.5        |\n",
      "|    reward             | -0.53746796 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 2.77        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 552       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.4     |\n",
      "|    explained_variance | -0.0918   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -33.5     |\n",
      "|    reward             | 0.1475914 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.892     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 548        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 27.9       |\n",
      "|    reward             | 0.30520862 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.75       |\n",
      "--------------------------------------\n",
      "day: 626, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 862360.06\n",
      "total_reward: -137639.94\n",
      "total_cost: 4902.33\n",
      "total_trades: 9142\n",
      "Sharpe: -0.286\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 547      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 105      |\n",
      "|    reward             | 2.441135 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 8.28     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 547         |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 57          |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.4       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | -40.7       |\n",
      "|    reward             | -0.34931323 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.969       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 546        |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | -119       |\n",
      "|    reward             | 0.25658444 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 8.73       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 546        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 10.4       |\n",
      "|    reward             | -0.7746536 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.266      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 546         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | 42.7        |\n",
      "|    reward             | -0.36529934 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 1.54        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 546        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | -127       |\n",
      "|    reward             | 0.58577913 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 9.44       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 546         |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | 9.35        |\n",
      "|    reward             | -0.36613014 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.521       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 546         |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 63          |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.7       |\n",
      "|    explained_variance | -41.8       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | -56.6       |\n",
      "|    reward             | 0.013035366 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 2.73        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 546       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 64.2      |\n",
      "|    reward             | 1.4253347 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2.55      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 545        |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | -29.3      |\n",
      "|    reward             | 0.34130207 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 545        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.9      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | 49.3       |\n",
      "|    reward             | -0.7188276 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 545        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 33.5       |\n",
      "|    reward             | -1.2102308 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n",
      "day: 626, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 913113.11\n",
      "total_reward: -86886.89\n",
      "total_cost: 6629.34\n",
      "total_trades: 8953\n",
      "Sharpe: -0.147\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 545       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | 0.0998    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 43.4      |\n",
      "|    reward             | 0.0707048 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 544        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -2.57      |\n",
      "|    reward             | -1.1611795 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.205      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 544        |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | -56.3      |\n",
      "|    reward             | -0.2360697 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.94       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 544       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | 82.3      |\n",
      "|    reward             | 1.349495  |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 5.27      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 543        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 18         |\n",
      "|    reward             | -1.6755828 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.607      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 542        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 72         |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -90.8      |\n",
      "|    reward             | 0.44578052 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 4.62       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 542         |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 73          |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.2       |\n",
      "|    explained_variance | 2.38e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | -138        |\n",
      "|    reward             | -0.02060472 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 11.5        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 542         |\n",
      "|    iterations         | 8100        |\n",
      "|    time_elapsed       | 74          |\n",
      "|    total_timesteps    | 40500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.2       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8099        |\n",
      "|    policy_loss        | -93.2       |\n",
      "|    reward             | -0.13426422 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 5.54        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 542       |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | 7.29      |\n",
      "|    reward             | 0.6735553 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.185     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 541          |\n",
      "|    iterations         | 8300         |\n",
      "|    time_elapsed       | 76           |\n",
      "|    total_timesteps    | 41500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -45.2        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8299         |\n",
      "|    policy_loss        | -92.5        |\n",
      "|    reward             | -0.026159145 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 4.45         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 541        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 77         |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | 72.2       |\n",
      "|    reward             | -1.1392999 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 3.49       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 541         |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -94.4       |\n",
      "|    reward             | -0.31092045 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 6.78        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 540        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 38.1       |\n",
      "|    reward             | -0.4909454 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.773      |\n",
      "--------------------------------------\n",
      "day: 626, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 984325.31\n",
      "total_reward: -15674.69\n",
      "total_cost: 6092.41\n",
      "total_trades: 8968\n",
      "Sharpe: 0.055\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 540        |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 17.1       |\n",
      "|    reward             | 0.22221017 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.25       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 540         |\n",
      "|    iterations         | 8800        |\n",
      "|    time_elapsed       | 81          |\n",
      "|    total_timesteps    | 44000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.4       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8799        |\n",
      "|    policy_loss        | 11.7        |\n",
      "|    reward             | -0.45797744 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 0.104       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -35.8     |\n",
      "|    reward             | 0.9889604 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.846     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 540        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 78.8       |\n",
      "|    reward             | 0.23064058 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 4.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | 70.1      |\n",
      "|    reward             | 1.1536987 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.91      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 539        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | -77.6      |\n",
      "|    reward             | -2.3235013 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 4.16       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 539         |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 86          |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.4       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | 27.7        |\n",
      "|    reward             | -0.46992102 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 0.579       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 539         |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 87          |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 11.6        |\n",
      "|    reward             | -0.40032375 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 0.447       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 9.84      |\n",
      "|    reward             | 0.3735326 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.29      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 539         |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 89          |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.6       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | -12.8       |\n",
      "|    reward             | -0.83409864 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 1.4         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 6.41      |\n",
      "|    reward             | -0.462728 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.648     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 90          |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.6       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -94.4       |\n",
      "|    reward             | -0.48906347 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 5.65        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 91          |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | -4.47       |\n",
      "|    reward             | 0.052061528 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.826       |\n",
      "---------------------------------------\n",
      "day: 626, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 942007.58\n",
      "total_reward: -57992.42\n",
      "total_cost: 4638.14\n",
      "total_trades: 9908\n",
      "Sharpe: -0.061\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | -40.3      |\n",
      "|    reward             | -3.3082938 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 2.84       |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:42:56.051316Z",
     "start_time": "2024-09-20T08:42:56.040977Z"
    }
   },
   "cell_type": "code",
   "source": "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None",
   "id": "939a4eb8878b69ad",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 2: DDPG\n",
   "id": "76ad1315a9f77f1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:42:56.112659Z",
     "start_time": "2024-09-20T08:42:56.052324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)\n"
   ],
   "id": "80414b32fb00b3ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:48:06.347166Z",
     "start_time": "2024-09-20T08:42:56.113667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ],
   "id": "a39d4af118749420",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 164       |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 2508      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05     |\n",
      "|    critic_loss     | 18.6      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2407      |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 165       |\n",
      "|    time_elapsed    | 30        |\n",
      "|    total_timesteps | 5016      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 5.22      |\n",
      "|    critic_loss     | 1.31      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4915      |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "day: 626, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1181349.13\n",
      "total_reward: 181349.13\n",
      "total_cost: 1055.82\n",
      "total_trades: 8764\n",
      "Sharpe: 0.506\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 168       |\n",
      "|    time_elapsed    | 44        |\n",
      "|    total_timesteps | 7524      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.92      |\n",
      "|    critic_loss     | 0.472     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7423      |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 59        |\n",
      "|    total_timesteps | 10032     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 4.3       |\n",
      "|    critic_loss     | 1.11e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 9931      |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "day: 626, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1181349.13\n",
      "total_reward: 181349.13\n",
      "total_cost: 1055.82\n",
      "total_trades: 8764\n",
      "Sharpe: 0.506\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 73        |\n",
      "|    total_timesteps | 12540     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.91     |\n",
      "|    critic_loss     | 0.17      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 12439     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 90        |\n",
      "|    total_timesteps | 15048     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.35     |\n",
      "|    critic_loss     | 0.155     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 14947     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 164       |\n",
      "|    time_elapsed    | 106       |\n",
      "|    total_timesteps | 17556     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.597    |\n",
      "|    critic_loss     | 0.0722    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 17455     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "day: 626, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1181349.13\n",
      "total_reward: 181349.13\n",
      "total_cost: 1055.82\n",
      "total_trades: 8764\n",
      "Sharpe: 0.506\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 163       |\n",
      "|    time_elapsed    | 122       |\n",
      "|    total_timesteps | 20064     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.37     |\n",
      "|    critic_loss     | 0.153     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 19963     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 161       |\n",
      "|    time_elapsed    | 139       |\n",
      "|    total_timesteps | 22572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.43     |\n",
      "|    critic_loss     | 0.0296    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 22471     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "day: 626, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1181349.13\n",
      "total_reward: 181349.13\n",
      "total_cost: 1055.82\n",
      "total_trades: 8764\n",
      "Sharpe: 0.506\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 160       |\n",
      "|    time_elapsed    | 155       |\n",
      "|    total_timesteps | 25080     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.34     |\n",
      "|    critic_loss     | 0.0546    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 24979     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 160       |\n",
      "|    time_elapsed    | 171       |\n",
      "|    total_timesteps | 27588     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.681    |\n",
      "|    critic_loss     | 0.0151    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 27487     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 160       |\n",
      "|    time_elapsed    | 187       |\n",
      "|    total_timesteps | 30096     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.707    |\n",
      "|    critic_loss     | 0.288     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 29995     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "day: 626, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1181349.13\n",
      "total_reward: 181349.13\n",
      "total_cost: 1055.82\n",
      "total_trades: 8764\n",
      "Sharpe: 0.506\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 160       |\n",
      "|    time_elapsed    | 203       |\n",
      "|    total_timesteps | 32604     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.75     |\n",
      "|    critic_loss     | 0.371     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 32503     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 56        |\n",
      "|    fps             | 160       |\n",
      "|    time_elapsed    | 218       |\n",
      "|    total_timesteps | 35112     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.473    |\n",
      "|    critic_loss     | 0.329     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 35011     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "day: 626, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1181349.13\n",
      "total_reward: 181349.13\n",
      "total_cost: 1055.82\n",
      "total_trades: 8764\n",
      "Sharpe: 0.506\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 60        |\n",
      "|    fps             | 160       |\n",
      "|    time_elapsed    | 233       |\n",
      "|    total_timesteps | 37620     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.26     |\n",
      "|    critic_loss     | 0.00787   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 37519     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 64        |\n",
      "|    fps             | 161       |\n",
      "|    time_elapsed    | 249       |\n",
      "|    total_timesteps | 40128     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.76     |\n",
      "|    critic_loss     | 0.0136    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 40027     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 161       |\n",
      "|    time_elapsed    | 264       |\n",
      "|    total_timesteps | 42636     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14     |\n",
      "|    critic_loss     | 0.0116    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 42535     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "day: 626, episode: 150\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1181349.13\n",
      "total_reward: 181349.13\n",
      "total_cost: 1055.82\n",
      "total_trades: 8764\n",
      "Sharpe: 0.506\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 161       |\n",
      "|    time_elapsed    | 280       |\n",
      "|    total_timesteps | 45144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.858    |\n",
      "|    critic_loss     | 0.301     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 45043     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 161       |\n",
      "|    time_elapsed    | 295       |\n",
      "|    total_timesteps | 47652     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05     |\n",
      "|    critic_loss     | 0.0178    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 47551     |\n",
      "|    reward          | 0.9440603 |\n",
      "----------------------------------\n",
      "day: 626, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1181349.13\n",
      "total_reward: 181349.13\n",
      "total_cost: 1055.82\n",
      "total_trades: 8764\n",
      "Sharpe: 0.506\n",
      "=================================\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:48:06.365937Z",
     "start_time": "2024-09-20T08:48:06.347166Z"
    }
   },
   "cell_type": "code",
   "source": "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None",
   "id": "2fe4b79b37cc33df",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 3: PPO",
   "id": "c74c497ed894bc73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:48:06.376842Z",
     "start_time": "2024-09-20T08:48:06.365937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ],
   "id": "59e03686e34b0d53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:53:11.378093Z",
     "start_time": "2024-09-20T08:48:06.376842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ],
   "id": "c21e44a53d9f2c58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 703          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 2            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.055938117 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 662         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013933469 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.117      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.11        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0379     |\n",
      "|    reward               | 0.8722053   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 6.73        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 170\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1039802.24\n",
      "total_reward: 39802.24\n",
      "total_cost: 134285.95\n",
      "total_trades: 16908\n",
      "Sharpe: 0.177\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 661         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018060558 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0532      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    reward               | 0.9458619   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.92        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0183969  |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0994     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 3.29       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    reward               | -1.2665689 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 6.41       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016202545 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.00253     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    reward               | 0.0219003   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 6.18        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1286440.42\n",
      "total_reward: 286440.42\n",
      "total_cost: 143270.48\n",
      "total_trades: 17106\n",
      "Sharpe: 0.710\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022066984 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0249      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.18        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    reward               | -0.5865891  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 5.48        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.017972512  |\n",
      "|    clip_fraction        | 0.224        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.9        |\n",
      "|    explained_variance   | 0.0868       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.61         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0228      |\n",
      "|    reward               | -0.026780885 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.91         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016441412 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.0472      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.06        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    reward               | 0.521173    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 5.95        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 190\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 956178.88\n",
      "total_reward: -43821.12\n",
      "total_cost: 136045.90\n",
      "total_trades: 16858\n",
      "Sharpe: -0.025\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023221105 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    reward               | 0.05126374  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 5.24        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 644         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019754099 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    reward               | -0.48523247 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 6.05        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012965395 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.58        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    reward               | 0.6361143   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 6.51        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 921849.58\n",
      "total_reward: -78150.42\n",
      "total_cost: 120088.12\n",
      "total_trades: 16226\n",
      "Sharpe: -0.106\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019968523 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.49        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    reward               | 0.6203178   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 5.7         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020068083 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.4         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    reward               | -0.95712644 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 5.78        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017872248 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.66        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    reward               | 0.38270366  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 5.16        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023779996 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    reward               | -0.15712881 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 6.18        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 210\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1070543.53\n",
      "total_reward: 70543.53\n",
      "total_cost: 126159.32\n",
      "total_trades: 16447\n",
      "Sharpe: 0.244\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021581374 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.71        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    reward               | 0.66959983  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 6.21        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019117525 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.44        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    reward               | -0.15047333 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 6.45        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019337699 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.81        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    reward               | 0.041851472 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 6.13        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 220\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1004710.46\n",
      "total_reward: 4710.46\n",
      "total_cost: 111006.51\n",
      "total_trades: 15954\n",
      "Sharpe: 0.101\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 60         |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02023891 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.4      |\n",
      "|    explained_variance   | 0.261      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.9        |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    reward               | 2.1751974  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 6.07       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 640         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02102143  |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    reward               | -0.52783036 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 5.47        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022995658 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    reward               | -0.4435975  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 5.07        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 230\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1095820.31\n",
      "total_reward: 95820.31\n",
      "total_cost: 125858.06\n",
      "total_trades: 16399\n",
      "Sharpe: 0.294\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024178723 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    reward               | -1.2825235  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 4.87        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021413889 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    reward               | -0.6583443  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 4.98        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022626437 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    reward               | 0.73542476  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 4.93        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 240\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 986179.10\n",
      "total_reward: -13820.90\n",
      "total_cost: 120633.35\n",
      "total_trades: 16343\n",
      "Sharpe: 0.050\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027922567 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    reward               | 0.096650295 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 5.26        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 639         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025743593 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    reward               | -1.0794256  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 5.09        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027702216 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    reward               | 0.4017418   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 250\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 944023.31\n",
      "total_reward: -55976.69\n",
      "total_cost: 116061.87\n",
      "total_trades: 16249\n",
      "Sharpe: -0.040\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 89         |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03314825 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.7      |\n",
      "|    explained_variance   | 0.321      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 3.2        |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    reward               | -1.7754369 |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 5.72       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 640         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034900755 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    reward               | 2.2505722   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 4.67        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 636         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023801222 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.53        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    reward               | -0.6261677  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 6.09        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 260\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1033733.35\n",
      "total_reward: 33733.35\n",
      "total_cost: 114849.83\n",
      "total_trades: 16159\n",
      "Sharpe: 0.164\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 636         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028006684 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.64        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | 0.17347486  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 5.2         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030363236 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    reward               | -2.8638518  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 4.46        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020274732 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.27        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    reward               | 0.11089073  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 4.78        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 270\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 985026.69\n",
      "total_reward: -14973.31\n",
      "total_cost: 110045.22\n",
      "total_trades: 15873\n",
      "Sharpe: 0.061\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025340462 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.28        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    reward               | 1.3276215   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 5.5         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028745063 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    reward               | -0.70525885 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026317326 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    reward               | -0.22262962 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 4.4         |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 280\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1166975.50\n",
      "total_reward: 166975.50\n",
      "total_cost: 123079.71\n",
      "total_trades: 16458\n",
      "Sharpe: 0.470\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027737219 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    reward               | -1.9900181  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 4.76        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027456904 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    reward               | 0.94970846  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 4.69        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029836021 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    reward               | -1.4235634  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 4.24        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 290\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1113400.42\n",
      "total_reward: 113400.42\n",
      "total_cost: 118332.25\n",
      "total_trades: 15968\n",
      "Sharpe: 0.326\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025279148 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    reward               | -1.143752   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 5           |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03354522  |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    reward               | -0.85132384 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 5.08        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028066952 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.81        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    reward               | 0.18521428  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 6.59        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 300\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1235868.81\n",
      "total_reward: 235868.81\n",
      "total_cost: 119243.45\n",
      "total_trades: 16133\n",
      "Sharpe: 0.582\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 636         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022357374 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.4         |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    reward               | -2.1466966  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 6.82        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051983148 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    reward               | 0.29758874  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 4.99        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 144        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03412859 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.8      |\n",
      "|    explained_variance   | 0.488      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 2.52       |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    reward               | -0.433305  |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 5.15       |\n",
      "----------------------------------------\n",
      "day: 626, episode: 310\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1172491.46\n",
      "total_reward: 172491.46\n",
      "total_cost: 105134.73\n",
      "total_trades: 15460\n",
      "Sharpe: 0.431\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 639         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027303899 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.608       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.47        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    reward               | -0.72438204 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 7.67        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 640         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025441362 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    reward               | 1.7363187   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 6.63        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028777186 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    reward               | -3.4911418  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 4.84        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 320\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1417633.94\n",
      "total_reward: 417633.94\n",
      "total_cost: 94348.17\n",
      "total_trades: 14899\n",
      "Sharpe: 0.780\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020419447 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.31        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    reward               | -0.31115997 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 5.9         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028759088 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | -0.31933948 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 5.51        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033519585 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    reward               | -2.4506712  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 5.18        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 330\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1172588.38\n",
      "total_reward: 172588.38\n",
      "total_cost: 122839.14\n",
      "total_trades: 16141\n",
      "Sharpe: 0.492\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024954408 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -0.15982093 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 5.37        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030529242 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    reward               | 0.14008784  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 4.84        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035876207 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    reward               | 1.7558922   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 4.03        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 340\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1055152.82\n",
      "total_reward: 55152.82\n",
      "total_cost: 117466.51\n",
      "total_trades: 15751\n",
      "Sharpe: 0.216\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 174        |\n",
      "|    total_timesteps      | 112640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03266936 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.6      |\n",
      "|    explained_variance   | 0.449      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.56       |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    reward               | 0.5673075  |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 3.73       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 644         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052941773 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.6       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.709       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | 0.19919711  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 644         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033628125 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00653    |\n",
      "|    reward               | -0.75479203 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 4.56        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 350\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1230853.84\n",
      "total_reward: 230853.84\n",
      "total_cost: 118756.13\n",
      "total_trades: 15906\n",
      "Sharpe: 0.634\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 184         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035665527 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    reward               | -0.62176913 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039961927 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.8       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    reward               | 2.8364432   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 4.64        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029903263 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.8       |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    reward               | -0.80484575 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 360\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1360647.20\n",
      "total_reward: 360647.20\n",
      "total_cost: 98931.03\n",
      "total_trades: 14882\n",
      "Sharpe: 0.826\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 192        |\n",
      "|    total_timesteps      | 124928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02690121 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.9      |\n",
      "|    explained_variance   | 0.487      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.93       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    reward               | 0.63432026 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 4.45       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023549974 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    reward               | 1.0936981   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040276073 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46         |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | 1.4624295   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 5.01        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 370\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1171622.57\n",
      "total_reward: 171622.57\n",
      "total_cost: 107019.53\n",
      "total_trades: 15380\n",
      "Sharpe: 0.492\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 201        |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0304592  |\n",
      "|    clip_fraction        | 0.306      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.1      |\n",
      "|    explained_variance   | 0.534      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.95       |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    reward               | 0.09278115 |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 4.35       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02620787  |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.1       |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    reward               | -0.12065992 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 4.96        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 208         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025420107 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.2       |\n",
      "|    explained_variance   | 0.49        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    reward               | -4.050677   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 5.69        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027316432 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.2       |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.65        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    reward               | 1.76773     |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 5.95        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 380\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1306039.95\n",
      "total_reward: 306039.95\n",
      "total_cost: 86794.62\n",
      "total_trades: 14190\n",
      "Sharpe: 0.674\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 68         |\n",
      "|    time_elapsed         | 214        |\n",
      "|    total_timesteps      | 139264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03093771 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.3      |\n",
      "|    explained_variance   | 0.589      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 2.61       |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    reward               | 2.256867   |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 5.42       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 217         |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025003875 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.3       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | 1.2303816   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 5.18        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01995353  |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.4       |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | -0.92733145 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 5.55        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 390\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1514306.35\n",
      "total_reward: 514306.35\n",
      "total_cost: 83341.97\n",
      "total_trades: 14244\n",
      "Sharpe: 1.061\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023248497 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.4       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.69        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00976    |\n",
      "|    reward               | 0.24526006  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 5.08        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 653          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 225          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.026456963  |\n",
      "|    clip_fraction        | 0.269        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.5        |\n",
      "|    explained_variance   | 0.731        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.9          |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    reward               | -0.112555854 |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 4.62         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033233725 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.41        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | 1.3203924   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 6.61        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 400\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1351882.74\n",
      "total_reward: 351882.74\n",
      "total_cost: 86243.20\n",
      "total_trades: 14270\n",
      "Sharpe: 0.778\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023393024 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.6       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    reward               | -0.5269539  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 5.36        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 654         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 234         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029481374 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.6       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    reward               | 0.49013805  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 5.69        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 654         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032120384 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.7       |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.85        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -0.74314874 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 8.83        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 410\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1255517.79\n",
      "total_reward: 255517.79\n",
      "total_cost: 72436.27\n",
      "total_trades: 13557\n",
      "Sharpe: 0.581\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 654         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037301708 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.8       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00825    |\n",
      "|    reward               | 1.9251251   |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 5.3         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 654         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021477055 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.9       |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    reward               | 0.70644224  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 5.27        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 655         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039399467 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.9       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00832    |\n",
      "|    reward               | 0.74324274  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 5           |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 420\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1610429.31\n",
      "total_reward: 610429.31\n",
      "total_cost: 98720.31\n",
      "total_trades: 14731\n",
      "Sharpe: 1.179\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 655         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022256767 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47         |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.44        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | 0.77844715  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 4.92        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 655        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 253        |\n",
      "|    total_timesteps      | 165888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02799221 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -47        |\n",
      "|    explained_variance   | 0.808      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.91       |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.00535   |\n",
      "|    reward               | -1.0107365 |\n",
      "|    std                  | 1.16       |\n",
      "|    value_loss           | 5.61       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 655         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023822142 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.1       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.97        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    reward               | 0.57679325  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 4.49        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 430\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1298720.67\n",
      "total_reward: 298720.67\n",
      "total_cost: 76683.22\n",
      "total_trades: 13921\n",
      "Sharpe: 0.642\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01818316  |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.2       |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | -0.10636447 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 5.14        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023706209 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.2       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | 1.5863456   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 4.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024764199 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.3       |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    reward               | 0.36565417  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 440\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1289474.47\n",
      "total_reward: 289474.47\n",
      "total_cost: 62711.93\n",
      "total_trades: 13219\n",
      "Sharpe: 0.620\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016246192 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.4       |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.77        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | 0.932593    |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 7.05        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 271         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024515053 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.5       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.08        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    reward               | -0.28887677 |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 4.83        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 656        |\n",
      "|    iterations           | 88         |\n",
      "|    time_elapsed         | 274        |\n",
      "|    total_timesteps      | 180224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01970036 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -47.5      |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.52       |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.00814   |\n",
      "|    reward               | -1.5223955 |\n",
      "|    std                  | 1.18       |\n",
      "|    value_loss           | 4.44       |\n",
      "----------------------------------------\n",
      "day: 626, episode: 450\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1432254.44\n",
      "total_reward: 432254.44\n",
      "total_cost: 70688.38\n",
      "total_trades: 13487\n",
      "Sharpe: 0.863\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 657         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 277         |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012601363 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.6       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.63        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    reward               | 0.7057557   |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 8.48        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 657         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034856673 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.6       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    reward               | -2.3569698  |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 657        |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 283        |\n",
      "|    total_timesteps      | 186368     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02202706 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -47.7      |\n",
      "|    explained_variance   | 0.869      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.73       |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    reward               | 0.939359   |\n",
      "|    std                  | 1.19       |\n",
      "|    value_loss           | 4.6        |\n",
      "----------------------------------------\n",
      "day: 626, episode: 460\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1291321.41\n",
      "total_reward: 291321.41\n",
      "total_cost: 65434.13\n",
      "total_trades: 13381\n",
      "Sharpe: 0.621\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 657         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023187272 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.7       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 0.89616233  |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 5.05        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 657         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019726891 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.8       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    reward               | 0.17492901  |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 657         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018912494 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.8       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 0.22492257  |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 470\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1389443.87\n",
      "total_reward: 389443.87\n",
      "total_cost: 64127.85\n",
      "total_trades: 13420\n",
      "Sharpe: 0.773\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024571124 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.8       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    reward               | 1.5888718   |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 3.93        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 298         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016465394 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.8       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00685    |\n",
      "|    reward               | -1.5632309  |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 3.58        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013421468 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.9       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | -5.6497984  |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 3.74        |\n",
      "-----------------------------------------\n",
      "day: 626, episode: 480\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1381350.20\n",
      "total_reward: 381350.20\n",
      "total_cost: 69438.03\n",
      "total_trades: 13707\n",
      "Sharpe: 0.773\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016232207 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.9       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | 0.3512905   |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 3.59        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:53:11.397383Z",
     "start_time": "2024-09-20T08:53:11.379102Z"
    }
   },
   "cell_type": "code",
   "source": "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None",
   "id": "784d57bc463a0532",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 4: TD3",
   "id": "9f80e39aa214723f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:53:11.459305Z",
     "start_time": "2024-09-20T08:53:11.398388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ],
   "id": "a36729ad348d631c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:58:21.508768Z",
     "start_time": "2024-09-20T08:53:11.459305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ],
   "id": "64a5a1c69501aea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 165       |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 2508      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 45.3      |\n",
      "|    critic_loss     | 2.67e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2407      |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "day: 626, episode: 490\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1193210.46\n",
      "total_reward: 193210.46\n",
      "total_cost: 999.00\n",
      "total_trades: 13146\n",
      "Sharpe: 0.491\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 165       |\n",
      "|    time_elapsed    | 30        |\n",
      "|    total_timesteps | 5016      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 29.8      |\n",
      "|    critic_loss     | 2.39      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4915      |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 45        |\n",
      "|    total_timesteps | 7524      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 24        |\n",
      "|    critic_loss     | 2.4       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7423      |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 59        |\n",
      "|    total_timesteps | 10032     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 28        |\n",
      "|    critic_loss     | 1.94      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 9931      |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "day: 626, episode: 500\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1193210.46\n",
      "total_reward: 193210.46\n",
      "total_cost: 999.00\n",
      "total_trades: 13146\n",
      "Sharpe: 0.491\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 168       |\n",
      "|    time_elapsed    | 74        |\n",
      "|    total_timesteps | 12540     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 26.8      |\n",
      "|    critic_loss     | 8.39      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 12439     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 90        |\n",
      "|    total_timesteps | 15048     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 25.9      |\n",
      "|    critic_loss     | 22.1      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 14947     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "day: 626, episode: 510\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1193210.46\n",
      "total_reward: 193210.46\n",
      "total_cost: 999.00\n",
      "total_trades: 13146\n",
      "Sharpe: 0.491\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 105       |\n",
      "|    total_timesteps | 17556     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 25.5      |\n",
      "|    critic_loss     | 2.75      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 17455     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 164       |\n",
      "|    time_elapsed    | 121       |\n",
      "|    total_timesteps | 20064     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 24.9      |\n",
      "|    critic_loss     | 0.964     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 19963     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 164       |\n",
      "|    time_elapsed    | 137       |\n",
      "|    total_timesteps | 22572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 22.7      |\n",
      "|    critic_loss     | 0.22      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 22471     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "day: 626, episode: 520\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1193210.46\n",
      "total_reward: 193210.46\n",
      "total_cost: 999.00\n",
      "total_trades: 13146\n",
      "Sharpe: 0.491\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 163       |\n",
      "|    time_elapsed    | 153       |\n",
      "|    total_timesteps | 25080     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 21.5      |\n",
      "|    critic_loss     | 0.542     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 24979     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 163       |\n",
      "|    time_elapsed    | 169       |\n",
      "|    total_timesteps | 27588     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 19.9      |\n",
      "|    critic_loss     | 0.61      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 27487     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "day: 626, episode: 530\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1193210.46\n",
      "total_reward: 193210.46\n",
      "total_cost: 999.00\n",
      "total_trades: 13146\n",
      "Sharpe: 0.491\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 162       |\n",
      "|    time_elapsed    | 184       |\n",
      "|    total_timesteps | 30096     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 19.3      |\n",
      "|    critic_loss     | 0.438     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 29995     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 163       |\n",
      "|    time_elapsed    | 200       |\n",
      "|    total_timesteps | 32604     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 17.5      |\n",
      "|    critic_loss     | 0.242     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 32503     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 56        |\n",
      "|    fps             | 162       |\n",
      "|    time_elapsed    | 216       |\n",
      "|    total_timesteps | 35112     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 18.2      |\n",
      "|    critic_loss     | 0.439     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 35011     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "day: 626, episode: 540\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1193210.46\n",
      "total_reward: 193210.46\n",
      "total_cost: 999.00\n",
      "total_trades: 13146\n",
      "Sharpe: 0.491\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 60        |\n",
      "|    fps             | 161       |\n",
      "|    time_elapsed    | 232       |\n",
      "|    total_timesteps | 37620     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 18.3      |\n",
      "|    critic_loss     | 0.176     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 37519     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 64        |\n",
      "|    fps             | 162       |\n",
      "|    time_elapsed    | 247       |\n",
      "|    total_timesteps | 40128     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 17.5      |\n",
      "|    critic_loss     | 0.133     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 40027     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "day: 626, episode: 550\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1193210.46\n",
      "total_reward: 193210.46\n",
      "total_cost: 999.00\n",
      "total_trades: 13146\n",
      "Sharpe: 0.491\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 162       |\n",
      "|    time_elapsed    | 263       |\n",
      "|    total_timesteps | 42636     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 15.1      |\n",
      "|    critic_loss     | 0.181     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 42535     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 162       |\n",
      "|    time_elapsed    | 278       |\n",
      "|    total_timesteps | 45144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 15.4      |\n",
      "|    critic_loss     | 0.105     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 45043     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 161       |\n",
      "|    time_elapsed    | 294       |\n",
      "|    total_timesteps | 47652     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 14.8      |\n",
      "|    critic_loss     | 0.254     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 47551     |\n",
      "|    reward          | 0.9769124 |\n",
      "----------------------------------\n",
      "day: 626, episode: 560\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1193210.46\n",
      "total_reward: 193210.46\n",
      "total_cost: 999.00\n",
      "total_trades: 13146\n",
      "Sharpe: 0.491\n",
      "=================================\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:58:21.538309Z",
     "start_time": "2024-09-20T08:58:21.509773Z"
    }
   },
   "cell_type": "code",
   "source": "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None",
   "id": "5f8ca6e945c8d961",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 5: SAC",
   "id": "acfaf4252e1a15c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:58:21.625122Z",
     "start_time": "2024-09-20T08:58:21.539315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ],
   "id": "55d1ae66e4e39c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T09:07:56.293872Z",
     "start_time": "2024-09-20T08:58:21.626128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ],
   "id": "741367b8c5cee421",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 119       |\n",
      "|    time_elapsed    | 20        |\n",
      "|    total_timesteps | 2508      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 55.4      |\n",
      "|    critic_loss     | 63.9      |\n",
      "|    ent_coef        | 0.097     |\n",
      "|    ent_coef_loss   | -117      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 2407      |\n",
      "|    reward          | 1.0255969 |\n",
      "----------------------------------\n",
      "day: 626, episode: 570\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1096255.39\n",
      "total_reward: 96255.39\n",
      "total_cost: 76078.19\n",
      "total_trades: 16380\n",
      "Sharpe: 0.294\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 119       |\n",
      "|    time_elapsed    | 42        |\n",
      "|    total_timesteps | 5016      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 40.9      |\n",
      "|    critic_loss     | 625       |\n",
      "|    ent_coef        | 0.0754    |\n",
      "|    ent_coef_loss   | -128      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 4915      |\n",
      "|    reward          | 1.1132003 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 122       |\n",
      "|    time_elapsed    | 61        |\n",
      "|    total_timesteps | 7524      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 17.7      |\n",
      "|    critic_loss     | 9.03      |\n",
      "|    ent_coef        | 0.0586    |\n",
      "|    ent_coef_loss   | -140      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 7423      |\n",
      "|    reward          | 0.6401599 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 124       |\n",
      "|    time_elapsed    | 80        |\n",
      "|    total_timesteps | 10032     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.82     |\n",
      "|    critic_loss     | 1.64      |\n",
      "|    ent_coef        | 0.0457    |\n",
      "|    ent_coef_loss   | -152      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 9931      |\n",
      "|    reward          | 0.6833337 |\n",
      "----------------------------------\n",
      "day: 626, episode: 580\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 706325.85\n",
      "total_reward: -293674.15\n",
      "total_cost: 37893.34\n",
      "total_trades: 13281\n",
      "Sharpe: -0.599\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 20         |\n",
      "|    fps             | 125        |\n",
      "|    time_elapsed    | 99         |\n",
      "|    total_timesteps | 12540      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -8.38      |\n",
      "|    critic_loss     | 3.11       |\n",
      "|    ent_coef        | 0.0355     |\n",
      "|    ent_coef_loss   | -164       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 12439      |\n",
      "|    reward          | 0.58004224 |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 125       |\n",
      "|    time_elapsed    | 120       |\n",
      "|    total_timesteps | 15048     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -14.5     |\n",
      "|    critic_loss     | 1.12      |\n",
      "|    ent_coef        | 0.0277    |\n",
      "|    ent_coef_loss   | -175      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 14947     |\n",
      "|    reward          | 0.6268918 |\n",
      "----------------------------------\n",
      "day: 626, episode: 590\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 734996.60\n",
      "total_reward: -265003.40\n",
      "total_cost: 12046.65\n",
      "total_trades: 11354\n",
      "Sharpe: -0.507\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 124       |\n",
      "|    time_elapsed    | 140       |\n",
      "|    total_timesteps | 17556     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -18.9     |\n",
      "|    critic_loss     | 1.78      |\n",
      "|    ent_coef        | 0.0216    |\n",
      "|    ent_coef_loss   | -182      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 17455     |\n",
      "|    reward          | 0.6705391 |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 32         |\n",
      "|    fps             | 124        |\n",
      "|    time_elapsed    | 160        |\n",
      "|    total_timesteps | 20064      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -14.8      |\n",
      "|    critic_loss     | 87.5       |\n",
      "|    ent_coef        | 0.0168     |\n",
      "|    ent_coef_loss   | -190       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 19963      |\n",
      "|    reward          | 0.74833006 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 36         |\n",
      "|    fps             | 124        |\n",
      "|    time_elapsed    | 181        |\n",
      "|    total_timesteps | 22572      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -16.4      |\n",
      "|    critic_loss     | 0.737      |\n",
      "|    ent_coef        | 0.0131     |\n",
      "|    ent_coef_loss   | -192       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 22471      |\n",
      "|    reward          | 0.81334865 |\n",
      "-----------------------------------\n",
      "day: 626, episode: 600\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 794600.45\n",
      "total_reward: -205399.55\n",
      "total_cost: 6964.85\n",
      "total_trades: 10771\n",
      "Sharpe: -0.344\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 124       |\n",
      "|    time_elapsed    | 201       |\n",
      "|    total_timesteps | 25080     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -15.7     |\n",
      "|    critic_loss     | 1.9       |\n",
      "|    ent_coef        | 0.0103    |\n",
      "|    ent_coef_loss   | -198      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 24979     |\n",
      "|    reward          | 0.8731518 |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 44         |\n",
      "|    fps             | 124        |\n",
      "|    time_elapsed    | 221        |\n",
      "|    total_timesteps | 27588      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -14.2      |\n",
      "|    critic_loss     | 0.52       |\n",
      "|    ent_coef        | 0.00802    |\n",
      "|    ent_coef_loss   | -198       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 27487      |\n",
      "|    reward          | 0.88695556 |\n",
      "-----------------------------------\n",
      "day: 626, episode: 610\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 841777.57\n",
      "total_reward: -158222.43\n",
      "total_cost: 4871.35\n",
      "total_trades: 10646\n",
      "Sharpe: -0.250\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 48         |\n",
      "|    fps             | 124        |\n",
      "|    time_elapsed    | 242        |\n",
      "|    total_timesteps | 30096      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -12        |\n",
      "|    critic_loss     | 0.579      |\n",
      "|    ent_coef        | 0.00627    |\n",
      "|    ent_coef_loss   | -202       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 29995      |\n",
      "|    reward          | 0.76014423 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 52         |\n",
      "|    fps             | 123        |\n",
      "|    time_elapsed    | 263        |\n",
      "|    total_timesteps | 32604      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -9.38      |\n",
      "|    critic_loss     | 0.98       |\n",
      "|    ent_coef        | 0.00489    |\n",
      "|    ent_coef_loss   | -208       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 32503      |\n",
      "|    reward          | 0.74909574 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 56         |\n",
      "|    fps             | 123        |\n",
      "|    time_elapsed    | 285        |\n",
      "|    total_timesteps | 35112      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -6.72      |\n",
      "|    critic_loss     | 0.977      |\n",
      "|    ent_coef        | 0.00382    |\n",
      "|    ent_coef_loss   | -208       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 35011      |\n",
      "|    reward          | 0.69695395 |\n",
      "-----------------------------------\n",
      "day: 626, episode: 620\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 930541.36\n",
      "total_reward: -69458.64\n",
      "total_cost: 3834.43\n",
      "total_trades: 10847\n",
      "Sharpe: -0.070\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 60        |\n",
      "|    fps             | 123       |\n",
      "|    time_elapsed    | 305       |\n",
      "|    total_timesteps | 37620     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5        |\n",
      "|    critic_loss     | 3.77      |\n",
      "|    ent_coef        | 0.00299   |\n",
      "|    ent_coef_loss   | -205      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 37519     |\n",
      "|    reward          | 0.7433744 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 64        |\n",
      "|    fps             | 123       |\n",
      "|    time_elapsed    | 324       |\n",
      "|    total_timesteps | 40128     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.75     |\n",
      "|    critic_loss     | 0.471     |\n",
      "|    ent_coef        | 0.00234   |\n",
      "|    ent_coef_loss   | -201      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 40027     |\n",
      "|    reward          | 0.8498177 |\n",
      "----------------------------------\n",
      "day: 626, episode: 630\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1010066.59\n",
      "total_reward: 10066.59\n",
      "total_cost: 3640.06\n",
      "total_trades: 12237\n",
      "Sharpe: 0.109\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 123       |\n",
      "|    time_elapsed    | 344       |\n",
      "|    total_timesteps | 42636     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.24     |\n",
      "|    critic_loss     | 0.712     |\n",
      "|    ent_coef        | 0.00182   |\n",
      "|    ent_coef_loss   | -209      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 42535     |\n",
      "|    reward          | 0.8510394 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 123       |\n",
      "|    time_elapsed    | 364       |\n",
      "|    total_timesteps | 45144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.0747   |\n",
      "|    critic_loss     | 2.25      |\n",
      "|    ent_coef        | 0.00143   |\n",
      "|    ent_coef_loss   | -206      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 45043     |\n",
      "|    reward          | 1.0091665 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 123       |\n",
      "|    time_elapsed    | 385       |\n",
      "|    total_timesteps | 47652     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.25      |\n",
      "|    critic_loss     | 0.725     |\n",
      "|    ent_coef        | 0.00112   |\n",
      "|    ent_coef_loss   | -193      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 47551     |\n",
      "|    reward          | 0.8718569 |\n",
      "----------------------------------\n",
      "day: 626, episode: 640\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1088203.95\n",
      "total_reward: 88203.95\n",
      "total_cost: 2636.74\n",
      "total_trades: 13653\n",
      "Sharpe: 0.283\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 80         |\n",
      "|    fps             | 122        |\n",
      "|    time_elapsed    | 407        |\n",
      "|    total_timesteps | 50160      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 2.82       |\n",
      "|    critic_loss     | 0.286      |\n",
      "|    ent_coef        | 0.000879   |\n",
      "|    ent_coef_loss   | -177       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 50059      |\n",
      "|    reward          | 0.97537565 |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 84        |\n",
      "|    fps             | 122       |\n",
      "|    time_elapsed    | 428       |\n",
      "|    total_timesteps | 52668     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.65      |\n",
      "|    critic_loss     | 0.231     |\n",
      "|    ent_coef        | 0.000693  |\n",
      "|    ent_coef_loss   | -153      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 52567     |\n",
      "|    reward          | 1.0657042 |\n",
      "----------------------------------\n",
      "day: 626, episode: 650\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1185711.04\n",
      "total_reward: 185711.04\n",
      "total_cost: 1831.42\n",
      "total_trades: 13417\n",
      "Sharpe: 0.502\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 88        |\n",
      "|    fps             | 122       |\n",
      "|    time_elapsed    | 449       |\n",
      "|    total_timesteps | 55176     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 4.12      |\n",
      "|    critic_loss     | 0.227     |\n",
      "|    ent_coef        | 0.000549  |\n",
      "|    ent_coef_loss   | -141      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 55075     |\n",
      "|    reward          | 1.1426165 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 92        |\n",
      "|    fps             | 122       |\n",
      "|    time_elapsed    | 470       |\n",
      "|    total_timesteps | 57684     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 4.96      |\n",
      "|    critic_loss     | 0.194     |\n",
      "|    ent_coef        | 0.000437  |\n",
      "|    ent_coef_loss   | -108      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 57583     |\n",
      "|    reward          | 1.2430679 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 96        |\n",
      "|    fps             | 122       |\n",
      "|    time_elapsed    | 491       |\n",
      "|    total_timesteps | 60192     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 5.12      |\n",
      "|    critic_loss     | 0.16      |\n",
      "|    ent_coef        | 0.000349  |\n",
      "|    ent_coef_loss   | -80.6     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 60091     |\n",
      "|    reward          | 1.2210267 |\n",
      "----------------------------------\n",
      "day: 626, episode: 660\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1230394.44\n",
      "total_reward: 230394.44\n",
      "total_cost: 1643.35\n",
      "total_trades: 12966\n",
      "Sharpe: 0.592\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 100       |\n",
      "|    fps             | 122       |\n",
      "|    time_elapsed    | 512       |\n",
      "|    total_timesteps | 62700     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 4.37      |\n",
      "|    critic_loss     | 0.166     |\n",
      "|    ent_coef        | 0.000285  |\n",
      "|    ent_coef_loss   | -55.3     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 62599     |\n",
      "|    reward          | 1.2151365 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 104       |\n",
      "|    fps             | 122       |\n",
      "|    time_elapsed    | 533       |\n",
      "|    total_timesteps | 65208     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 6.43      |\n",
      "|    critic_loss     | 0.349     |\n",
      "|    ent_coef        | 0.000246  |\n",
      "|    ent_coef_loss   | -4.21     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 65107     |\n",
      "|    reward          | 1.3043427 |\n",
      "----------------------------------\n",
      "day: 626, episode: 670\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1237134.35\n",
      "total_reward: 237134.35\n",
      "total_cost: 1818.30\n",
      "total_trades: 12694\n",
      "Sharpe: 0.596\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 108       |\n",
      "|    fps             | 121       |\n",
      "|    time_elapsed    | 555       |\n",
      "|    total_timesteps | 67716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 6.59      |\n",
      "|    critic_loss     | 0.123     |\n",
      "|    ent_coef        | 0.000263  |\n",
      "|    ent_coef_loss   | 6.2       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 67615     |\n",
      "|    reward          | 1.3023562 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T09:07:56.315229Z",
     "start_time": "2024-09-20T09:07:56.294871Z"
    }
   },
   "cell_type": "code",
   "source": "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None",
   "id": "2681a3a365301f",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n"
   ],
   "id": "5bde4f1b483b0ebf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2. Backtesting",
   "id": "a450e337329af35a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
